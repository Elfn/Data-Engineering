{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe232fc-b1d6-4c2d-9be8-c313a7aad2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='01/02/1965', Time='13:44:18', Latitude='19.246', Longitude='145.616', Type='Earthquake', Depth='131.6', Depth Error=None, Depth Seismic Stations=None, Magnitude='6', Magnitude Type='MW', Magnitude Error=None, Magnitude Seismic Stations=None, Azimuthal Gap=None, Horizontal Distance=None, Horizontal Error=None, Root Mean Square=None, ID='ISCGEM860706', Source='ISCGEM', Location Source='ISCGEM', Magnitude Source='ISCGEM', Status='Automatic')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------EXTRACTION-------------------------------------\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Configure spark session\n",
    "spark = SparkSession.builder.master('local[2]').appName('quake_etl').config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:2.4.1').getOrCreate()\n",
    "\n",
    "# Load the dataset \"database.csv\"\n",
    "df_load = spark.read.csv(r\"/Users/Elimane/SPARK/data/database.csv\", header=True)\n",
    "#Preview df_load\n",
    "df_load.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13ee84f-e3dd-402d-a915-8013ce6ceab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop fields we don't need from df_load\n",
    "lst_dropped_columns = ['Depth Error', 'Time', 'Depth Seismic Stations','Magnitude Error','Magnitude Seismic Stations','Azimuthal Gap', 'Horizontal Distance','Horizontal Error',\n",
    "    'Root Mean Square','Source','Location Source','Magnitude Source','Status']\n",
    "df_load = df_load.drop(*lst_dropped_columns)\n",
    "#Preview df_load\n",
    "df_load.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a768cd-e9a3-4437-a326-342eaa4ba6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|        6|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake|   80|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake|   20|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake|   15|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake|   15|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create year fields and add it to dataframe\n",
    "df_load = df_load.withColumn('Year', year(to_timestamp('Date', 'dd/MM/yyyy')))\n",
    "#Preview df_load\n",
    "#df_load = df_load.drop('Day','')\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b79934-619f-4ea6-8cff-e5d3d37ebe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Count|\n",
      "+----+-----+\n",
      "|1990|  196|\n",
      "|1975|  150|\n",
      "|1977|  148|\n",
      "|2003|  187|\n",
      "|2007|  211|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Build the quake frequency dataframe using the year field\n",
    "df_quake_freq = df_load.groupBy('Year').count().withColumnRenamed('count', 'Count')\n",
    "#Preview df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80cfb5c1-180b-4f0b-94a4-1ec7178d4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------TRANSFORMATION-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1f2c32-63c8-4195-a45f-ae97a2e2621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast some fields from string into numeric types\n",
    "df_load = df_load.withColumn('Latitude', df_load['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_load['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_load['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_load['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "# Preview df_load\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c078ef8b-9572-4b5c-9734-df369ae9c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Magnitude Type: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_load schema\n",
    "df_load.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b864f163-a9d6-45b2-a11a-ecba0312225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create avg magnitude and max magnitude fields and add to df_quake_freq\n",
    "df_max = df_load.groupBy('Year').max('Magnitude').withColumnRenamed('max(Magnitude)', 'Max_Magnitude')\n",
    "df_avg = df_load.groupBy('Year').avg('Magnitude').withColumnRenamed('avg(Magnitude)', 'Avg_Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66abb55f-d348-4501-9365-b97b174c3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------------+-------------+\n",
      "|Year|Count|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+-----+-----------------+-------------+\n",
      "|1990|  196|5.858163265306125|          7.6|\n",
      "|1975|  150| 5.84866666666667|          7.8|\n",
      "|1977|  148|5.757432432432437|          7.6|\n",
      "|2003|  187|5.850802139037435|          7.6|\n",
      "|2007|  211| 5.89099526066351|          8.4|\n",
      "+----+-----+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join df_max, and df_avg to df_quake_freq\n",
    "df_quake_freq = df_quake_freq.join(df_avg, ['Year']).join(df_max, ['Year'])\n",
    "# Preview df_quake_freq\n",
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413d2b01-9425-439f-9670-e0e39655e7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Count: bigint, Avg_Magnitude: double, Max_Magnitude: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove nulls\n",
    "df_load.dropna()\n",
    "df_quake_freq.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d294444f-3e69-4b4a-afe1-5e0b4f179c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|      Date|Latitude|Longitude|      Type|Depth|Magnitude|Magnitude Type|          ID|Year|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "|01/02/1965|  19.246|  145.616|Earthquake|131.6|      6.0|            MW|ISCGEM860706|1965|\n",
      "|01/04/1965|   1.863|  127.352|Earthquake| 80.0|      5.8|            MW|ISCGEM860737|1965|\n",
      "|01/05/1965| -20.579| -173.972|Earthquake| 20.0|      6.2|            MW|ISCGEM860762|1965|\n",
      "|01/08/1965| -59.076|  -23.557|Earthquake| 15.0|      5.8|            MW|ISCGEM860856|1965|\n",
      "|01/09/1965|  11.938|  126.427|Earthquake| 15.0|      5.8|            MW|ISCGEM860890|1965|\n",
      "+----------+--------+---------+----------+-----+---------+--------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview dataframes\n",
    "df_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54f363d-b548-4e5c-8c8f-33681100d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------------+-------------+\n",
      "|Year|Count|    Avg_Magnitude|Max_Magnitude|\n",
      "+----+-----+-----------------+-------------+\n",
      "|1990|  196|5.858163265306125|          7.6|\n",
      "|1975|  150| 5.84866666666667|          7.8|\n",
      "|1977|  148|5.757432432432437|          7.6|\n",
      "|2003|  187|5.850802139037435|          7.6|\n",
      "|2007|  211| 5.89099526066351|          8.4|\n",
      "+----+-----+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quake_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b797dc7d-4da2-4c86-92e6-0730efbe83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------LOADING-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b90be4b-8af5-4c6e-b5a6-ac720c18a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Build the tables or collections\n",
    "# Write df_load to mongodb\n",
    "df_load.write.format('mongo').mode('overwrite').option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe696cd-b5cd-48f7-985a-8b8104f16e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write df_quake_freq to mongodb\n",
    "df_quake_freq.write.format('mongo').mode('overwrite').option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quake_freq').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d94e23-12f6-45ec-b243-b653c79078cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSection Machine Learning Part\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Section Machine Learning Part\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36cc128c-9065-48b6-a085-c0cc54ec3090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(time='2017-01-02T00:13:06.300Z', latitude='-36.0365', longitude='51.9288', depth='10', mag='5.7', magType='mwb', nst=None, gap='26', dmin='14.685', rms='1.37', net='us', id='us10007p5d', updated='2017-03-27T23:53:17.040Z', place='Southwest Indian Ridge', type='earthquake', horizontalError='10.3', depthError='1.7', magError='0.068', magNst='21', status='reviewed', locationSource='us', magSource='us'),\n",
       " Row(time='2017-01-02T13:13:48.710Z', latitude='-4.895', longitude='-76.3675', depth='106', mag='5.9', magType='mww', nst=None, gap='31', dmin='3.002', rms='0.82', net='us', id='us10007p7n', updated='2020-01-02T23:56:16.978Z', place='37km E of Barranca, Peru', type='earthquake', horizontalError='7.1', depthError='1.9', magError=None, magNst=None, status='reviewed', locationSource='us', magSource='us')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the test data into a dataframe\n",
    "df_test = spark.read.csv(r\"/Users/Elimane/SPARK/data/query.csv\", header=True)\n",
    "\n",
    "# Preview df_test\n",
    "df_test.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed04d9c-26a1-4671-9779-e5fa0e4b3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+-----------+----------------------+-------------------+----------------+------------+--------+---------------+---------+---------+---------------+--------------------------+----------------+--------------+----------------+------+---------+--------+----------+--------------------+\n",
      "|Azimuthal Gap|      Date|Depth|Depth Error|Depth Seismic Stations|Horizontal Distance|Horizontal Error|          ID|Latitude|Location Source|Longitude|Magnitude|Magnitude Error|Magnitude Seismic Stations|Magnitude Source|Magnitude Type|Root Mean Square|Source|   Status|    Time|      Type|                 _id|\n",
      "+-------------+----------+-----+-----------+----------------------+-------------------+----------------+------------+--------+---------------+---------+---------+---------------+--------------------------+----------------+--------------+----------------+------+---------+--------+----------+--------------------+\n",
      "|         null|01/02/1965|131.6|       null|                  null|               null|            null|ISCGEM860706|  19.246|         ISCGEM|  145.616|        6|           null|                      null|          ISCGEM|            MW|            null|ISCGEM|Automatic|13:44:18|Earthquake|{61ce3b392102a77e...|\n",
      "|         null|01/04/1965|   80|       null|                  null|               null|            null|ISCGEM860737|   1.863|         ISCGEM|  127.352|      5.8|           null|                      null|          ISCGEM|            MW|            null|ISCGEM|Automatic|11:29:49|Earthquake|{61ce3b392102a77e...|\n",
      "|         null|01/05/1965|   20|       null|                  null|               null|            null|ISCGEM860762| -20.579|         ISCGEM| -173.972|      6.2|           null|                      null|          ISCGEM|            MW|            null|ISCGEM|Automatic|18:05:58|Earthquake|{61ce3b392102a77e...|\n",
      "|         null|01/08/1965|   15|       null|                  null|               null|            null|ISCGEM860856| -59.076|         ISCGEM|  -23.557|      5.8|           null|                      null|          ISCGEM|            MW|            null|ISCGEM|Automatic|18:49:43|Earthquake|{61ce3b392102a77e...|\n",
      "|         null|01/09/1965|   15|       null|                  null|               null|            null|ISCGEM860890|  11.938|         ISCGEM|  126.427|      5.8|           null|                      null|          ISCGEM|            MW|            null|ISCGEM|Automatic|13:32:50|Earthquake|{61ce3b392102a77e...|\n",
      "+-------------+----------+-----+-----------+----------------------+-------------------+----------------+------------+--------+---------------+---------+---------+---------------+--------------------------+----------------+--------------+----------------+------+---------+--------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training data from mongo\n",
    "df_train = spark.read.format('mongo').option('spark.mongodb.output.uri', 'mongodb://127.0.0.1:27017/Quake.quakes').load()\n",
    "\n",
    "# Preview df_train\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a2de54-e2d3-4b30-8807-8dc37bd18e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---+-----+\n",
      "|                time|latitude|longitude|mag|depth|\n",
      "+--------------------+--------+---------+---+-----+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|5.7|   10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|5.9|  106|\n",
      "+--------------------+--------+---------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select field we will use and discard fields we don't need\n",
    "df_test_cleaned = df_test['time','latitude','longitude','mag','depth']\n",
    "\n",
    "# Preview df_test_cleaned\n",
    "df_test_cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f4c543-767e-49e0-bdf8-8c05b0fec31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+---------+-----+\n",
      "|                Date|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------------------+--------+---------+---------+-----+\n",
      "|2017-01-02T00:13:...|-36.0365|  51.9288|      5.7|   10|\n",
      "|2017-01-02T13:13:...|  -4.895| -76.3675|      5.9|  106|\n",
      "+--------------------+--------+---------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename fields\n",
    "df_test_cleaned = df_test_cleaned.withColumnRenamed('time', 'Date')\\\n",
    "    .withColumnRenamed('latitude', 'Latitude')\\\n",
    "    .withColumnRenamed('longitude', 'Longitude')\\\n",
    "    .withColumnRenamed('mag', 'Magnitude')\\\n",
    "    .withColumnRenamed('depth', 'Depth')\\\n",
    "\n",
    "# Preview df_test_cleaned\n",
    "df_test_cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d92ed5f5-451a-4b61-bbf6-f9bf3b820046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Magnitude: string (nullable = true)\n",
      " |-- Depth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview sSchema\n",
    "df_test_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45f3bbf5-b647-44ed-9f76-e6272d59de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Depth: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast some string fields into numeric fields\n",
    "df_test_cleaned = df_test_cleaned.withColumn('Latitude', df_test_cleaned['Latitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Longitude', df_test_cleaned['Longitude'].cast(DoubleType()))\\\n",
    "    .withColumn('Depth', df_test_cleaned['Depth'].cast(DoubleType()))\\\n",
    "    .withColumn('Magnitude', df_test_cleaned['Magnitude'].cast(DoubleType()))\n",
    "\n",
    "\n",
    "# Preview df_test_cleaned Schema\n",
    "df_test_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "126ae37a-4d63-43c0-b1e8-6d4143e82e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing and training dataframes\n",
    "df_testing = df_test_cleaned['Latitude','Longitude','Magnitude','Depth']\n",
    "df_training = df_train['Latitude','Longitude','Magnitude','Depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "712f325c-29d1-4ced-85ea-cd1b150716c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-----+\n",
      "|Latitude|Longitude|Magnitude|Depth|\n",
      "+--------+---------+---------+-----+\n",
      "|  19.246|  145.616|        6|131.6|\n",
      "|   1.863|  127.352|      5.8|   80|\n",
      "| -20.579| -173.972|      6.2|   20|\n",
      "| -59.076|  -23.557|      5.8|   15|\n",
      "|  11.938|  126.427|      5.8|   15|\n",
      "+--------+---------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_training\n",
    "df_training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16b4ed4d-6073-4e68-b47e-de10f1fa7829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+\n",
      "|Latitude|Longitude|Magnitude| Depth|\n",
      "+--------+---------+---------+------+\n",
      "|-36.0365|  51.9288|      5.7|  10.0|\n",
      "|  -4.895| -76.3675|      5.9| 106.0|\n",
      "|-23.2513| 179.2383|      6.3|551.62|\n",
      "| 24.0151|  92.0177|      5.7|  32.0|\n",
      "|-43.3527| -74.5017|      5.5| 10.26|\n",
      "+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview df_testing\n",
    "df_testing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36f7dc0e-5e29-4788-ad20-9ececc9e72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with null values from our dataframes\n",
    "df_testing = df_testing.dropna()\n",
    "df_training = df_training.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f888be46-11e9-4eeb-b8d3-fd98509d20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------ML MODELS------------------------------------------------------------\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4066ed4-10f2-4a26-9507-0a6189d87ce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column Latitude is not supported.\nData type string of column Longitude is not supported.\nData type string of column Depth is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cl/g1h08_md5sq33g04jlxf3g100000gn/T/ipykernel_98417/3633284082.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Make the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SPARK/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type string of column Latitude is not supported.\nData type string of column Longitude is not supported.\nData type string of column Depth is not supported."
     ]
    }
   ],
   "source": [
    "\n",
    "# Select features to parse into our model and then create the feature vector\n",
    "assembler = VectorAssembler(inputCols=['Latitude','Longitude','Depth'], outputCol='features')\n",
    "\n",
    "# Create the model\n",
    "model_reg = RandomForestRegressor(featuresCol='features', labelCol='Magnitude')\n",
    "\n",
    "# Chain the assembler with the model in a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, model_reg])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(df_training)\n",
    "\n",
    "# Make the prediction\n",
    "pred_results = model.transform(df_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81220-7e05-467b-8370-c9490749e89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
